{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PhD and Supervisor data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean and restructure dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import pandas as pd\n",
    "\n",
    "# custom functions\n",
    "from src.clean_names_helpers import remove_non_person_contributors_and_export, format_name_to_lastname_firstname, ensure_and_load_spacy_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically reloads any modules that are imported, \n",
    "# so that any changes made to the module files are reflected # without needing to restart the Jupyter kernel.\n",
    "# load autoreload module\n",
    "%load_ext autoreload\n",
    "# mode 1 reloads only when an import statement is called. For production\n",
    "# mode 2 reloads before execution of every cell\n",
    "%autoreload 2\n",
    "\n",
    "# limit the number of rows that are shown with printing data frames\n",
    "pd.set_option('display.max_rows', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load, and if not available, download the spacy nlp model \n",
    "model_name = \"xx_ent_wiki_sm\" # multilingual NER model\n",
    "nlp = ensure_and_load_spacy_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize values\n",
    "NROWS = None # None for all\n",
    "\n",
    "# names that spacy does not recognize as such\n",
    "# NOTE: Add the verbatim names here, not the standardized target notation \n",
    "# This list can be fed from removed_contributors.csv that is created when running the script\n",
    "WHITELIST = [ \n",
    "    \"Oosterlaan, J.\",\n",
    "    \"Nollet, F.\"\n",
    "    ] \n",
    "\n",
    "# non-people's names that don't get filtered out by spaCy \n",
    "BLACKLIST = [\n",
    "    \"Cardiology\"\n",
    "]\n",
    "\n",
    "removed_contributors = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "pairs_raw = pd.read_csv(\"data/raw/pairs_sups_phds.csv\", nrows=NROWS)\n",
    "pairs_raw = pairs_raw.convert_dtypes() # make sure all integer columns are integer dtype \n",
    "pairs_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates\n",
    "pairs_filtered = pairs_raw.drop_duplicates() \n",
    "\n",
    "# Remove rows where 'contributor' is NA\n",
    "pairs_filtered = pairs_filtered.dropna(subset=['contributor'])\n",
    "\n",
    "# remove contributors that aren't people\n",
    "csv_path = \"data/removed_contributors.csv\"\n",
    "pairs_filtered = remove_non_person_contributors_and_export(pairs_filtered, csv_path, nlp, WHITELIST, BLACKLIST)\n",
    "\n",
    "print(f\"{len(pairs_filtered)} columns are left.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize names\n",
    "pairs_std = pairs_filtered.copy()\n",
    "# Apply name standardization to the contributor column\n",
    "pairs_std['contributor'] = pairs_filtered['contributor'].apply(format_name_to_lastname_firstname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by publication\n",
    "aggregated = pairs_std.groupby([\n",
    "        'integer_id', \n",
    "        'thesis_identifier', \n",
    "        'institution', \n",
    "        'author_name', \n",
    "        'title', \n",
    "        'year', \n",
    "        'language'\n",
    "    ])\n",
    "        \n",
    "# Aggregate contributors into a list\n",
    "aggregated = aggregated.agg(list)\n",
    "\n",
    "aggregated = aggregated.reset_index()\n",
    "    \n",
    "# make sure the contributor is a sequence from 1 to n_contributors\n",
    "aggregated['contributor_order'] = aggregated['contributor_order'].apply(lambda lst: list(range(1, len(lst) + 1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot the dataset, to get to one row per dissertation, with the contributors in columns\n",
    "\n",
    "# Initialize a list to hold publication data dictionaries\n",
    "pubs_list = []\n",
    "\n",
    "# Iterate over each aggregated group\n",
    "for _, row in aggregated.iterrows():\n",
    "    # Initialize a dictionary with publication information\n",
    "    pub_dict = {col: row[col] for col in ['integer_id', 'thesis_identifier', 'institution', 'author_name', 'title', 'year', 'language']}\n",
    "    \n",
    "    # Get the list of contributors and their orders for this publication\n",
    "    contributors = row['contributor']\n",
    "    contributor_orders = row['contributor_order']\n",
    "    \n",
    "    # Add contributors to the dictionary using dynamic keys\n",
    "    for order in sorted(set(contributor_orders)):  # Ensure unique and sorted order numbers\n",
    "        if order - 1 < len(contributors):  # Check to prevent index error\n",
    "            pub_dict[f'contributor_{order}'] = contributors[order - 1]\n",
    "    \n",
    "    # Append the publication dictionary to the list\n",
    "    pubs_list.append(pub_dict)\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "pubs = pd.DataFrame(pubs_list).reset_index(drop=True)\n",
    "\n",
    "# Ensure correct data types and fill missing values with a suitable placeholder if necessary\n",
    "pubs = pubs.convert_dtypes()\n",
    "\n",
    "len(pubs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export a sample of the dataset to easily share it\n",
    "seed = 42 # fixed seed\n",
    "\n",
    "# Sample 100 random lines from the DataFrame\n",
    "#sampled_pubs = pubs.sample(n=50, random_state=seed)\n",
    "\n",
    "# Export the sampled DataFrame to a CSV file\n",
    "#sampled_pubs.to_csv('data/cleaned/sampled_pubs.csv', index=False)\n",
    "\n",
    "# Export the sampled DataFrame to a CSV file\n",
    "pubs.rename(columns={\"author_name\": \"phd_name\"}, inplace=True)\n",
    "\n",
    "pubs.to_csv('data/cleaned/pubs.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
