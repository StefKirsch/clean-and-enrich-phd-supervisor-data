{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The project is based on NARCIS (National Academic Research and Collaboration Information System) harvested meta-data from different PhD thesis repositories in The Netherlands (NARCIS, n.d.). NARCIS aggregates the information on scholarly work across Dutch academic institutions. NARCIS followed standardized protocols during the extraction, ensuring consistency and interoperability across various data sources.\n",
    "\n",
    "By now the NARCIS dataset has been included into OpenAIRE.\n",
    "\n",
    "To obtain the original dataset, please contact @tamarinde [on Github](https://github.com/tamarinde).\n",
    "\n",
    "This script is intended to clean the original dataset that was obtained from the XXX study. The raw dataset should be placed in\n",
    "\n",
    "`data/raw/pairs_sups_phds.csv`\n",
    "\n",
    "Make sure that the columns have the following names: `thesis_identifier`,`contributor`,`contributor_order`,`institution`,`author_name`,`title`,`year`,`language`\n",
    "\n",
    "This notebook cleans the raw data in the following ways\n",
    "\n",
    "1. Remove wrongly detected conributor entries that are not people. For this, we make use of spacy and the `xx_ent_wiki_sm` model\n",
    "2. Standardize the contributor names to \"Last name, Initials\" -> let's check here if we are actually throwing away information here and if we should keep the first names\n",
    "3. Aggregate all rows per publication/PhD student into one row with all the contributors\n",
    "4. Pivot the dataset, to get to one row per publication/PhD student, with one columns per contributor\n",
    "5. Export the dataset to `data/cleaned/sampled_pubs.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean and restructure dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import pandas as pd\n",
    "\n",
    "# custom functions\n",
    "from src.clean_names_helpers import remove_non_person_contributors_and_export\n",
    "from src.clean_names_helpers import format_name_to_lastname_initials\n",
    "from src.clean_names_helpers import ensure_and_load_spacy_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load, and if notvailable, download the spacy nlp model \n",
    "model_name = \"xx_ent_wiki_sm\" # multilingual NER model\n",
    "nlp = ensure_and_load_spacy_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize values\n",
    "NROWS = 100 # None for all\n",
    "\n",
    "# names that spacy does not recognize as such\n",
    "# NOTE: Add the verbatim names here, not the standardized target notation \n",
    "# This list can be fed from removed_contributors.csv that is created when running the script\n",
    "WHITELIST = [ \n",
    "    \"Oosterlaan, J.\",\n",
    "    \"Nollet, F.\"\n",
    "    ] \n",
    "\n",
    "# non-people's names that don't get filtered out by spaCy \n",
    "BLACKLIST = [\n",
    "    \"Cardiology\"\n",
    "]\n",
    "\n",
    "removed_contributors = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "pairs_raw = pd.read_csv(\"data/raw/pairs_sups_phds.csv\", nrows=NROWS)\n",
    "pairs_raw = pairs_raw.convert_dtypes() # make sure all integer columns are integer dtype \n",
    "pairs_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates\n",
    "pairs_filtered = pairs_raw.drop_duplicates() \n",
    "\n",
    "# Remove rows where 'contributor' is NA\n",
    "pairs_filtered = pairs_filtered.dropna(subset=['contributor'])\n",
    "\n",
    "# remove contributors that aren't people\n",
    "csv_path = \"data/removed_contributors.csv\"\n",
    "pairs_filtered = remove_non_person_contributors_and_export(pairs_filtered, csv_path, nlp, WHITELIST, BLACKLIST)\n",
    "\n",
    "print(f\"{len(pairs_filtered)} columns are left.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize names\n",
    "pairs_std = pairs_filtered\n",
    "# Apply name standardization to the contributor column\n",
    "pairs_std['contributor'] = pairs_filtered['contributor'].apply(format_name_to_lastname_initials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by publication\n",
    "aggregated = pairs_std.groupby([\n",
    "        'integer_id', \n",
    "        'thesis_identifier', \n",
    "        'institution', \n",
    "        'author_name', \n",
    "        'title', \n",
    "        'year', \n",
    "        'language'\n",
    "    ])\n",
    "        \n",
    "# Aggregate contributors into a list\n",
    "aggregated = aggregated.agg(list)\n",
    "\n",
    "aggregated = aggregated.reset_index()\n",
    "    \n",
    "# make sure the contributor is a sequence from 1 to n_contributors\n",
    "aggregated['contributor_order'] = aggregated['contributor_order'].apply(lambda lst: list(range(1, len(lst) + 1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot the dataset, to get to one row per dissertation, with the contributors in columns\n",
    "\n",
    "# Initialize a list to hold publication data dictionaries\n",
    "pubs_list = []\n",
    "\n",
    "# Iterate over each aggregated group\n",
    "for _, row in aggregated.iterrows():\n",
    "    # Initialize a dictionary with publication information\n",
    "    pub_dict = {col: row[col] for col in ['integer_id', 'thesis_identifier', 'institution', 'author_name', 'title', 'year', 'language']}\n",
    "    \n",
    "    # Get the list of contributors and their orders for this publication\n",
    "    contributors = row['contributor']\n",
    "    contributor_orders = row['contributor_order']\n",
    "    \n",
    "    # Add contributors to the dictionary using dynamic keys\n",
    "    for order in sorted(set(contributor_orders)):  # Ensure unique and sorted order numbers\n",
    "        if order - 1 < len(contributors):  # Check to prevent index error\n",
    "            pub_dict[f'contributor_{order}'] = contributors[order - 1]\n",
    "    \n",
    "    # Append the publication dictionary to the list\n",
    "    pubs_list.append(pub_dict)\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "pubs = pd.DataFrame(pubs_list)\n",
    "\n",
    "# Ensure correct data types and fill missing values with a suitable placeholder if necessary\n",
    "pubs = pubs.convert_dtypes()\n",
    "\n",
    "len(pubs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export a sample of the dataset to easily share it\n",
    "seed = 42 # fixed seed\n",
    "\n",
    "# Sample 100 random lines from the DataFrame\n",
    "#sampled_pubs = pubs.sample(n=50, random_state=seed)\n",
    "\n",
    "# Export the sampled DataFrame to a CSV file\n",
    "#sampled_pubs.to_csv('data/cleaned/sampled_pubs.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call Scopus API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.api_helpers as api_helpers\n",
    "\n",
    "from src.api_helpers import find_publications\n",
    "from src.api_helpers import find_first_publication\n",
    "from src.api_helpers import common_pub_author_and_contributor_1_row\n",
    "\n",
    "import importlib\n",
    "importlib.reload(api_helpers)\n",
    "\n",
    "# to find the configuration file, run\n",
    "# import pybliometrics\n",
    "# pybliometrics.scopus.utils.constants.CONFIG_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if I have the connection, in case my authentication does not work\n",
    "from pybliometrics.scopus import ScopusSearch\n",
    "s = ScopusSearch('ISSN(1532-849X) AND PUBYEAR IS 2010',subscriber=False)\n",
    "print(s.results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look up publications of one contributor\n",
    "author_last_name = \"van Neerven\"\n",
    "author_initials = \"J\"\n",
    "publications = find_publications(author_last_name, author_initials)\n",
    "\n",
    "publications.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pubs_sample = pubs.head(1)\n",
    "pubs_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go through a test set of the publications\n",
    "\n",
    "#common_pub_author_and_contributor_1_row(pubs_sample)\n",
    "\n",
    "author = pubs_sample['author_name'].iloc[0]\n",
    "contributor = pubs_sample['contributor_1'].iloc[0]\n",
    "\n",
    "publications_author = find_publications(*author.split(', ')) if author is not None else []\n",
    "publications_contributor = find_publications(*contributor.split(', ')) if contributor is not None else []\n",
    "\n",
    "# # Look up both author_name and contributor_1 and find the first common publication, if there is any\n",
    "# pubs_sample['common_publication'] = pubs_sample.apply(common_pub_author_and_contributor_1_row, axis=1)\n",
    "# pubs_sample\n",
    "\n",
    "contributor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, so good. The next step would be to get the first author each that the API returns and sees if they both share a publication. If that's not the case, the code should move to the next match for author_name, then contributor_1 and so on. The added layer of iteration is somewhat looking for plausible matches, namely matches that share a publication. We might still have a lot of false positives, this way though.\n",
    "\n",
    "Another layer we can build in is to verify the affilliation of the authors (that is in the matching publication). We can then whitelist papers where the PhD name is listed under the correct affiliation on that paper. As a stand-in we can also only count contributor_1 that had the same affiliation. To be determined if this works.\n",
    "\n",
    "We can also get some diagnostics on how many name matches we got per name in the dataframe. This should give us some idea on how many API calls we need to do and how certain we are with the name matching. \n",
    "We should also check how much of TiU's call budget we would us up to go throught the entire list. \n",
    "\n",
    "There seems to be an affiliation ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
