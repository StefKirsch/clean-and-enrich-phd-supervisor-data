{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PhD and Supervisor data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean and restructure dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import pandas as pd\n",
    "\n",
    "# custom functions\n",
    "from src.clean_names_helpers import remove_non_person_contributors_and_export\n",
    "from src.clean_names_helpers import format_name_to_lastname_initials\n",
    "from src.clean_names_helpers import ensure_and_load_spacy_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load, and if notvailable, download the spacy nlp model \n",
    "model_name = \"xx_ent_wiki_sm\" # multilingual NER model\n",
    "nlp = ensure_and_load_spacy_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize values\n",
    "NROWS = 100 # None for all\n",
    "\n",
    "# names that spacy does not recognize as such\n",
    "# NOTE: Add the verbatim names here, not the standardized target notation \n",
    "# This list can be fed from removed_contributors.csv that is created when running the script\n",
    "WHITELIST = [ \n",
    "    \"Oosterlaan, J.\",\n",
    "    \"Nollet, F.\"\n",
    "    ] \n",
    "\n",
    "# non-people's names that don't get filtered out by spaCy \n",
    "BLACKLIST = [\n",
    "    \"Cardiology\"\n",
    "]\n",
    "\n",
    "removed_contributors = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "pairs_raw = pd.read_csv(\"data/raw/pairs_sups_phds.csv\", nrows=NROWS)\n",
    "pairs_raw = pairs_raw.convert_dtypes() # make sure all integer columns are integer dtype \n",
    "pairs_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates\n",
    "pairs_filtered = pairs_raw.drop_duplicates() \n",
    "\n",
    "# Remove rows where 'contributor' is NA\n",
    "pairs_filtered = pairs_filtered.dropna(subset=['contributor'])\n",
    "\n",
    "# remove contributors that aren't people\n",
    "csv_path = \"data/removed_contributors.csv\"\n",
    "pairs_filtered = remove_non_person_contributors_and_export(pairs_filtered, csv_path, nlp, WHITELIST, BLACKLIST)\n",
    "\n",
    "print(f\"{len(pairs_filtered)} columns are left.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize names\n",
    "pairs_std = pairs_filtered\n",
    "# Apply name standardization to the contributor column\n",
    "pairs_std['contributor'] = pairs_filtered['contributor'].apply(format_name_to_lastname_initials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by publication\n",
    "aggregated = pairs_std.groupby([\n",
    "        'integer_id', \n",
    "        'thesis_identifier', \n",
    "        'institution', \n",
    "        'author_name', \n",
    "        'title', \n",
    "        'year', \n",
    "        'language'\n",
    "    ])\n",
    "        \n",
    "# Aggregate contributors into a list\n",
    "aggregated = aggregated.agg(list)\n",
    "\n",
    "aggregated = aggregated.reset_index()\n",
    "    \n",
    "# make sure the contributor is a sequence from 1 to n_contributors\n",
    "aggregated['contributor_order'] = aggregated['contributor_order'].apply(lambda lst: list(range(1, len(lst) + 1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot the dataset, to get to one row per dissertation, with the contributors in columns\n",
    "\n",
    "# Initialize a list to hold publication data dictionaries\n",
    "pubs_list = []\n",
    "\n",
    "# Iterate over each aggregated group\n",
    "for _, row in aggregated.iterrows():\n",
    "    # Initialize a dictionary with publication information\n",
    "    pub_dict = {col: row[col] for col in ['integer_id', 'thesis_identifier', 'institution', 'author_name', 'title', 'year', 'language']}\n",
    "    \n",
    "    # Get the list of contributors and their orders for this publication\n",
    "    contributors = row['contributor']\n",
    "    contributor_orders = row['contributor_order']\n",
    "    \n",
    "    # Add contributors to the dictionary using dynamic keys\n",
    "    for order in sorted(set(contributor_orders)):  # Ensure unique and sorted order numbers\n",
    "        if order - 1 < len(contributors):  # Check to prevent index error\n",
    "            pub_dict[f'contributor_{order}'] = contributors[order - 1]\n",
    "    \n",
    "    # Append the publication dictionary to the list\n",
    "    pubs_list.append(pub_dict)\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "pubs = pd.DataFrame(pubs_list)\n",
    "\n",
    "# Ensure correct data types and fill missing values with a suitable placeholder if necessary\n",
    "pubs = pubs.convert_dtypes()\n",
    "\n",
    "pubs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export a sample of the dataset to easily share it\n",
    "seed = 42 # fixed seed\n",
    "\n",
    "# Sample 100 random lines from the DataFrame\n",
    "#sampled_pubs = pubs.sample(n=50, random_state=seed)\n",
    "\n",
    "# Export the sampled DataFrame to a CSV file\n",
    "#sampled_pubs.to_csv('data/cleaned/sampled_pubs.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call Scopus API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "from src.api_helpers import find_publications\n",
    "from src.api_helpers import find_first_publication\n",
    "from src.api_helpers import common_pub_author_and_contributor_1_row\n",
    "\n",
    "# to find the configuration file, run\n",
    "# import pybliometrics\n",
    "# pybliometrics.scopus.utils.constants.CONFIG_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example call\n",
    "author_last_name = \"van Neerven\"\n",
    "author_initials = \"J\"\n",
    "publications = find_publications(author_last_name, author_initials)\n",
    "\n",
    "print(\"Publications:\", publications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if I have the connection, in case my authentication does not work\n",
    "from pybliometrics import ScopusSearch\n",
    "s = ScopusSearch('ISSN(1532-849X) AND PUBYEAR IS 2010',subscriber=False)\n",
    "print(s.results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go through all publications\n",
    "# use * to unpack the return value of split into two arguments\n",
    "pubs['contr1_publication'] = pubs['contributor_1'].apply(lambda x: find_first_publication(*x.split(', ')) if pd.notnull(x) else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look up both author_name and contributor_1 and find the first common publication, if there is any\n",
    "pubs['common_publication'] = pubs.apply(common_pub_author_and_contributor_1_row, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, so good. The next step would be to get the first author each that the API returns and sees if they both share a publication. If that's not the case, the code should move to the next match for author_name, then contributor_1 and so on. The added layer of iteration is somewhat looking for plausible matches, namely matches that share a publication. We might still have a lot of false positives, this way though.\n",
    "\n",
    "Another layer we can build in is to verify the affilliation of the authors (that is in the matching publication). We can then whitelist papers where the PhD name is listed under the correct affiliation on that paper. As a stand-in we can also only count contributor_1 that had the same affiliation. To be determined if this works.\n",
    "\n",
    "We can also get some diagnostics on how many name matches we got per name in the dataframe. This should give us some idea on how many API calls we need to do and how certain we are with the name matching. \n",
    "We should also check how much of TiU's call budget we would us up to go throught the entire list. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
