{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The project is based on NARCIS (National Academic Research and Collaboration Information System) harvested meta-data from different PhD thesis repositories in The Netherlands (NARCIS, n.d.). NARCIS aggregates the information on scholarly work across Dutch academic institutions. NARCIS followed standardized protocols during the extraction, ensuring consistency and interoperability across various data sources.\n",
    "\n",
    "By now the NARCIS dataset has been included into OpenAIRE.\n",
    "\n",
    "To obtain the original dataset, please contact @tamarinde [on Github](https://github.com/tamarinde).\n",
    "\n",
    "This script is intended to clean the original dataset that was obtained from the XXX study. The raw dataset should be placed in\n",
    "\n",
    "`data/raw/pairs_sups_phds.csv`\n",
    "\n",
    "Make sure that the columns have the following names: `thesis_identifier`,`contributor`,`contributor_order`,`institution`,`author_name`,`title`,`year`,`language`\n",
    "\n",
    "This notebook cleans the raw data in the following ways\n",
    "\n",
    "1. Remove wrongly detected conributor entries that are not people. For this, we make use of spacy and the `xx_ent_wiki_sm` model\n",
    "2. Standardize the contributor names to \"Last name, Initials\" -> let's check here if we are actually throwing away information here and if we should keep the first names\n",
    "3. Aggregate all rows per publication/PhD student into one row with all the contributors\n",
    "4. Pivot the dataset, to get to one row per publication/PhD student, with one columns per contributor\n",
    "5. Export the dataset to `data/cleaned/sampled_pubs.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import pandas as pd\n",
    "\n",
    "# custom functions\n",
    "from src.clean_names_helpers import remove_non_person_contributors_and_export\n",
    "from src.clean_names_helpers import format_name_to_lastname_initials\n",
    "from src.clean_names_helpers import ensure_and_load_spacy_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit the number of rows that are shown with printing dataframes\n",
    "pd.set_option('display.max_rows', 5)\n",
    "\n",
    "# Number of rows to read of the full dataset.\n",
    "NROWS = None # None for all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean and restructure dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load, and if notvailable, download the spacy nlp model \n",
    "model_name = \"xx_ent_wiki_sm\" # multilingual NER model\n",
    "nlp = ensure_and_load_spacy_model(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When checking if contributor names are actual names of people, the functions use a whitelist and a blacklist. If a name is on the whitelist, it will always be considered to be a name. Strings on the blacklist will however always be discared.\n",
    "\n",
    "The whitelist can be fed from the file `data/removed_contributors.csv` that is created when running the script.\n",
    "\n",
    "The blacklist can be fed from non-people names that end up in the filtered dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Names that spacy does not recognize as such\n",
    "# NOTE: Add the verbatim names here, not the standardized target notation \n",
    "\n",
    "WHITELIST = [ \n",
    "    \"Oosterlaan, J.\",\n",
    "    \"Nollet, F.\"\n",
    "    ] \n",
    "\n",
    "# non-people's names that don't get filtered out by spaCy \n",
    "BLACKLIST = [\n",
    "    \"Cardiology\"\n",
    "]\n",
    "\n",
    "removed_contributors = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "pairs_raw = pd.read_csv(\"data/raw/pairs_sups_phds.csv\", nrows=NROWS)\n",
    "pairs_raw = pairs_raw.convert_dtypes() # make sure all integer columns are integer dtype \n",
    "pairs_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove duplicates, rows where contributor is NA or not a person"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw data contains a lot of junk rows for contributors that the original scraper identified, but that are actually the institution, the reseach field or other junk.\n",
    "To remove contributors that are not people, we are using spacy with the model chosen above. The detecion still has a lot of false positives and negatives, but we can rid of them via a the custom `WHITELIST` and `BLACKLIST`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates\n",
    "pairs_filtered = pairs_raw.drop_duplicates() \n",
    "\n",
    "# Remove rows where 'contributor' is NA\n",
    "pairs_filtered = pairs_filtered.dropna(subset=['contributor'])\n",
    "\n",
    "# remove contributors that aren't people\n",
    "csv_path = \"data/removed_contributors.csv\"\n",
    "pairs_filtered = remove_non_person_contributors_and_export(pairs_filtered, csv_path, nlp, WHITELIST, BLACKLIST)\n",
    "\n",
    "print(f\"{len(pairs_filtered)} columns are left.\")\n",
    "pairs_filtered\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize names to Last name, Initials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize names\n",
    "pairs_std = pairs_filtered\n",
    "# Apply name standardization to the contributor column\n",
    "pairs_std['contributor'] = pairs_filtered['contributor'].apply(format_name_to_lastname_initials)\n",
    "\n",
    "pairs_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pivot\n",
    "\n",
    "Aggregate and unpack so that we get one row per publication and one column per contributor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by publication\n",
    "aggregated = pairs_std.groupby([\n",
    "        'integer_id', \n",
    "        'thesis_identifier', \n",
    "        'institution', \n",
    "        'author_name', \n",
    "        'title', \n",
    "        'year', \n",
    "        'language'\n",
    "    ])\n",
    "        \n",
    "# Aggregate contributors into a list\n",
    "aggregated = aggregated.agg(list)\n",
    "\n",
    "aggregated = aggregated.reset_index()\n",
    "    \n",
    "# make sure the contributor is a sequence from 1 to n_contributors\n",
    "aggregated['contributor_order'] = aggregated['contributor_order'].apply(lambda lst: list(range(1, len(lst) + 1)))\n",
    "\n",
    "aggregated\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unpack contributor rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot the dataset, to get to one row per dissertation, with the contributors in columns\n",
    "\n",
    "# Initialize a list to hold publication data dictionaries\n",
    "pubs_list = []\n",
    "\n",
    "# Iterate over each aggregated group\n",
    "for _, row in aggregated.iterrows():\n",
    "    # Initialize a dictionary with publication information\n",
    "    pub_dict = {col: row[col] for col in ['integer_id', 'thesis_identifier', 'institution', 'author_name', 'title', 'year', 'language']}\n",
    "    \n",
    "    # Get the list of contributors and their orders for this publication\n",
    "    contributors = row['contributor']\n",
    "    contributor_orders = row['contributor_order']\n",
    "    \n",
    "    # Add contributors to the dictionary using dynamic keys\n",
    "    for order in sorted(set(contributor_orders)):  # Ensure unique and sorted order numbers\n",
    "        if order - 1 < len(contributors):  # Check to prevent index error\n",
    "            pub_dict[f'contributor_{order}'] = contributors[order - 1]\n",
    "    \n",
    "    # Append the publication dictionary to the list\n",
    "    pubs_list.append(pub_dict)\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "pubs = pd.DataFrame(pubs_list)\n",
    "\n",
    "# Ensure correct data types and fill missing values with a suitable placeholder if necessary\n",
    "pubs = pubs.convert_dtypes()\n",
    "\n",
    "pubs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export cleaned dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pubs.to_csv('data/cleaned/pubs.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
