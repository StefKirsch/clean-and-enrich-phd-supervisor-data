{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PhD and Supervisor data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean and restructure dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import pandas as pd\n",
    "\n",
    "# custom functions\n",
    "from src.clean_names_helpers import remove_non_person_contributors_and_export\n",
    "from src.clean_names_helpers import format_name_to_lastname_firstname\n",
    "from src.clean_names_helpers import ensure_and_load_spacy_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically reloads any modules that are imported, \n",
    "# so that any changes made to the module files are reflected # without needing to restart the Jupyter kernel.\n",
    "# load autoreload module\n",
    "%load_ext autoreload\n",
    "# mode 1 reloads only when an import statement is called. For production\n",
    "# mode 2 reloads before execution of every cell\n",
    "%autoreload 2\n",
    "\n",
    "# limit the number of rows that are shown with printing dataframes\n",
    "pd.set_option('display.max_rows', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load, and if notvailable, download the spacy nlp model \n",
    "model_name = \"xx_ent_wiki_sm\" # multilingual NER model\n",
    "nlp = ensure_and_load_spacy_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize values\n",
    "NROWS = None # None for all\n",
    "\n",
    "# names that spacy does not recognize as such\n",
    "# NOTE: Add the verbatim names here, not the standardized target notation \n",
    "# This list can be fed from removed_contributors.csv that is created when running the script\n",
    "WHITELIST = [ \n",
    "    \"Oosterlaan, J.\",\n",
    "    \"Nollet, F.\"\n",
    "    ] \n",
    "\n",
    "# non-people's names that don't get filtered out by spaCy \n",
    "BLACKLIST = [\n",
    "    \"Cardiology\"\n",
    "]\n",
    "\n",
    "removed_contributors = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "pairs_raw = pd.read_csv(\"data/raw/pairs_sups_phds.csv\", nrows=NROWS)\n",
    "pairs_raw = pairs_raw.convert_dtypes() # make sure all integer columns are integer dtype \n",
    "pairs_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates\n",
    "pairs_filtered = pairs_raw.drop_duplicates() \n",
    "\n",
    "# Remove rows where 'contributor' is NA\n",
    "pairs_filtered = pairs_filtered.dropna(subset=['contributor'])\n",
    "\n",
    "# remove contributors that aren't people\n",
    "csv_path = \"data/removed_contributors.csv\"\n",
    "pairs_filtered = remove_non_person_contributors_and_export(pairs_filtered, csv_path, nlp, WHITELIST, BLACKLIST)\n",
    "\n",
    "print(f\"{len(pairs_filtered)} columns are left.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize names\n",
    "pairs_std = pairs_filtered\n",
    "# Apply name standardization to the contributor column\n",
    "pairs_std['contributor'] = pairs_filtered['contributor'].apply(format_name_to_lastname_firstname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by publication\n",
    "aggregated = pairs_std.groupby([\n",
    "        'integer_id', \n",
    "        'thesis_identifier', \n",
    "        'institution', \n",
    "        'author_name', \n",
    "        'title', \n",
    "        'year', \n",
    "        'language'\n",
    "    ])\n",
    "        \n",
    "# Aggregate contributors into a list\n",
    "aggregated = aggregated.agg(list)\n",
    "\n",
    "aggregated = aggregated.reset_index()\n",
    "    \n",
    "# make sure the contributor is a sequence from 1 to n_contributors\n",
    "aggregated['contributor_order'] = aggregated['contributor_order'].apply(lambda lst: list(range(1, len(lst) + 1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot the dataset, to get to one row per dissertation, with the contributors in columns\n",
    "\n",
    "# Initialize a list to hold publication data dictionaries\n",
    "pubs_list = []\n",
    "\n",
    "# Iterate over each aggregated group\n",
    "for _, row in aggregated.iterrows():\n",
    "    # Initialize a dictionary with publication information\n",
    "    pub_dict = {col: row[col] for col in ['integer_id', 'thesis_identifier', 'institution', 'author_name', 'title', 'year', 'language']}\n",
    "    \n",
    "    # Get the list of contributors and their orders for this publication\n",
    "    contributors = row['contributor']\n",
    "    contributor_orders = row['contributor_order']\n",
    "    \n",
    "    # Add contributors to the dictionary using dynamic keys\n",
    "    for order in sorted(set(contributor_orders)):  # Ensure unique and sorted order numbers\n",
    "        if order - 1 < len(contributors):  # Check to prevent index error\n",
    "            pub_dict[f'contributor_{order}'] = contributors[order - 1]\n",
    "    \n",
    "    # Append the publication dictionary to the list\n",
    "    pubs_list.append(pub_dict)\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "pubs = pd.DataFrame(pubs_list)\n",
    "\n",
    "# Ensure correct data types and fill missing values with a suitable placeholder if necessary\n",
    "pubs = pubs.convert_dtypes()\n",
    "\n",
    "len(pubs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export a sample of the dataset to easily share it\n",
    "seed = 42 # fixed seed\n",
    "\n",
    "# Sample 100 random lines from the DataFrame\n",
    "#sampled_pubs = pubs.sample(n=50, random_state=seed)\n",
    "\n",
    "# Export the sampled DataFrame to a CSV file\n",
    "#sampled_pubs.to_csv('data/cleaned/sampled_pubs.csv', index=False)\n",
    "\n",
    "# Export the sampled DataFrame to a CSV file\n",
    "pubs.to_csv('data/cleaned/pubs.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call Scopus API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.api_helpers as api_helpers\n",
    "\n",
    "from src.api_helpers import find_publications\n",
    "from src.api_helpers import find_first_publication\n",
    "from src.api_helpers import common_pub_author_and_contributor_1_row\n",
    "\n",
    "import importlib\n",
    "importlib.reload(api_helpers)\n",
    "\n",
    "# to find the configuration file, run\n",
    "# import pybliometrics\n",
    "# pybliometrics.scopus.utils.constants.CONFIG_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if I have the connection, in case my authentication does not work\n",
    "from pybliometrics.scopus import ScopusSearch\n",
    "s = ScopusSearch('ISSN(1532-849X) AND PUBYEAR IS 2010',subscriber=False)\n",
    "print(s.results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look up publications of one contributor\n",
    "author_last_name = \"van Neerven\"\n",
    "author_initials = \"J\"\n",
    "publications = find_publications(author_last_name, author_initials)\n",
    "\n",
    "publications.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pubs_sample = pubs.head(1)\n",
    "pubs_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go through a test set of the publications\n",
    "\n",
    "#common_pub_author_and_contributor_1_row(pubs_sample)\n",
    "\n",
    "author = pubs_sample['author_name'].iloc[0]\n",
    "contributor = pubs_sample['contributor_1'].iloc[0]\n",
    "\n",
    "publications_author = find_publications(*author.split(', ')) if author is not None else []\n",
    "publications_contributor = find_publications(*contributor.split(', ')) if contributor is not None else []\n",
    "\n",
    "# # Look up both author_name and contributor_1 and find the first common publication, if there is any\n",
    "# pubs_sample['common_publication'] = pubs_sample.apply(common_pub_author_and_contributor_1_row, axis=1)\n",
    "# pubs_sample\n",
    "\n",
    "contributor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, so good. The next step would be to get the first author each that the API returns and sees if they both share a publication. If that's not the case, the code should move to the next match for author_name, then contributor_1 and so on. The added layer of iteration is somewhat looking for plausible matches, namely matches that share a publication. We might still have a lot of false positives, this way though.\n",
    "\n",
    "Another layer we can build in is to verify the affilliation of the authors (that is in the matching publication). We can then whitelist papers where the PhD name is listed under the correct affiliation on that paper. As a stand-in we can also only count contributor_1 that had the same affiliation. To be determined if this works.\n",
    "\n",
    "We can also get some diagnostics on how many name matches we got per name in the dataframe. This should give us some idea on how many API calls we need to do and how certain we are with the name matching. \n",
    "We should also check how much of TiU's call budget we would us up to go throught the entire list. \n",
    "\n",
    "There seems to be an affiliation ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
