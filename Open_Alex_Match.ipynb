{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open Alex Extraction and Matching with .search()\n",
    "\n",
    "The goal of this Notebook is look up the PhD students (Authors) contained in the [cleaned](clean_data.ipynb) NARCIS dataset, and\n",
    "1. Confirm they can be found in OpenAlex\n",
    "2. Confirm their affiliation in NARCIS matches the one in OpenAlex\n",
    "2. Confirm they wrote the associated PhD Thesis\n",
    "3. Per author, look up all the contributors (i.e. potential first supervisors) that are listen in the NARCIS dataset and\n",
    "    a. Find all authors that have worked for the same organization at the time the PhD thesis was published (within a 1 year window)\n",
    "    b. xxx\n",
    "\n",
    "\n",
    "The previous version of this notebook written by a Bachelor student was using the `.search_filter()` method of `pyalex`, which does not search alternate spellings of the specified name. In this notebook we are using `search_filter()`, which does not have that problem. See the example code [here](search_parameter_vs_search_filter.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyalex import Works, Authors, Sources, Institutions, Topics, Publishers, Funders, Concepts\n",
    "import pyalex # importing full package seems to be the only way to call `pyalex.config.email = email_address`\n",
    "import pandas as pd\n",
    "from os import path\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from src.unabbreviate_institutions import unabbreviate_institutions\n",
    "from src.open_alex_helpers import AuthorRelations, find_phd_and_supervisors_in_row, get_supervisors_openalex_ids\n",
    "from src.io_helpers import remove_illegal_title_characters\n",
    "from src.clean_names_helpers import format_name_to_lastname_firstname\n",
    "from src.config_selector import get_dataset_selector\n",
    "\n",
    "# Initialize tqdm for progress bars\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and display the selector\n",
    "selector = get_dataset_selector()\n",
    "selector.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NROWS = selector.NROWS\n",
    "use_dataset = selector.use_dataset\n",
    "output_filename = selector.output_filename\n",
    "\n",
    "print(f\"Now using dataset: {use_dataset} with NROWS={NROWS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically reloads any modules that are imported, \n",
    "# so that any changes made to the module files are reflected # without needing to restart the Jupyter kernel.\n",
    "# load autoreload module\n",
    "%load_ext autoreload\n",
    "# mode 1 reloads only when an import statement is called. For production\n",
    "# mode 2 reloads before execution of every cell\n",
    "%autoreload 2\n",
    "\n",
    "# limit the number of rows that are shown with printing data frames\n",
    "pd.set_option('display.max_rows', 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set contact email address to get to use the [polite pool](https://docs.openalex.org/how-to-use-the-api/rate-limits-and-authentication#the-polite-pool). Also, if you are on a premium plan, you can access the higher usage limit by using the associated email address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get contact email address from file\n",
    "email_file_path = 'contact_email.txt'\n",
    "\n",
    "if path.isfile(email_file_path):\n",
    "    with open(email_file_path, 'r') as file:\n",
    "        email_address = file.read().strip()\n",
    "\n",
    "    # Assign the email address to the pyalex configuration\n",
    "    pyalex.config.email = email_address\n",
    "\n",
    "pyalex.config.email"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Cleaned processed NARCIS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if NROWS == None:\n",
    "    output_filename = 'data/output/matched_pairs_full.csv'\n",
    "else:\n",
    "    output_filename = f'data/output/matched_pairs_{NROWS}_rows.csv'\n",
    "\n",
    "if use_dataset == 'biomedical_5y':\n",
    "    # biomedical subset\n",
    "    pubs_df = pd.read_csv('data/cleaned/pubs_biomedical5y.csv')\n",
    "else:\n",
    "    # general dataset\n",
    "    pubs_df = pd.read_csv('data/cleaned/pubs.csv')\n",
    "\n",
    "# Take a sample\n",
    "\n",
    "if NROWS == None:\n",
    "    n_sample = len(pubs_df)\n",
    "else:\n",
    "    n_sample = NROWS\n",
    "\n",
    "pubs_df = pubs_df.sample(n=n_sample, random_state=42).reset_index(drop=True)\n",
    "\n",
    "pubs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace institution abbreviation with names that can be found in OpenAlex\n",
    "pubs_unabbrev_df = unabbreviate_institutions(pubs_df, 'institution')\n",
    "pubs_unabbrev_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Priority supervisor list from ResponsibleSupervision pilot\n",
    "\n",
    "This dataset was created during the Responsible Supervision pilot project, see [here](https://github.com/tamarinde/ResponsibleSupervision/tree/main/Pilot-responsible-supervision)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_url = \"https://github.com/tamarinde/ResponsibleSupervision/tree/main/Pilot-responsible-supervision/data/spreadsheets\"\n",
    "csv_path = \"data/output/sups_pilot.csv\"\n",
    "\n",
    "try:\n",
    "    # Attempt to read the supervisors in the pilot dataset from csv_path\n",
    "    # If it fails, we get them again from GitHub\n",
    "    supervisors_in_pilot_dataset = get_supervisors_openalex_ids(repo_url, csv_path)\n",
    "    print(\"Unique Supervisors with OpenAlex IDs:\")\n",
    "    print(supervisors_in_pilot_dataset)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the dict to overwrite the default class attribute specified in src/open_alex_helpers.py\n",
    "AuthorRelations.supervisors_in_pilot_dataset = supervisors_in_pilot_dataset\n",
    "\n",
    "# Apply the function to each row\n",
    "extraction_series = pubs_unabbrev_df.progress_apply(find_phd_and_supervisors_in_row, axis=1)\n",
    "\n",
    "# Concatenate all DataFrames into one\n",
    "extraction_df = pd.concat(list(extraction_series), ignore_index=True)\n",
    "\n",
    "extraction_df.to_csv(output_filename, index=False)\n",
    "\n",
    "extraction_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analysis and Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the extraction dataset from file in case we didn't run the extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'extraction_df' not in locals() and 'extraction_df' not in globals():\n",
    "    file_path = output_filename\n",
    "    \n",
    "    # Check if the file exists\n",
    "    if path.exists(file_path):\n",
    "        extraction_df = pd.read_csv(file_path)\n",
    "        print(f\"Read `extraction_df` from {file_path}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "extraction_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get PhDs that we could not find in OpenAlex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Filter extraction_df for rows with phd_id = NaN\n",
    "extraction_none_df = extraction_df.query(\"phd_id != phd_id\")\n",
    "\n",
    "# Step 2: Filter pubs_unabbrev_df for matching phd_names; then sort and export\n",
    "pubs_phd_not_confirmed_df = (\n",
    "    pubs_unabbrev_df\n",
    "    .query(\"phd_name in @extraction_none_df.phd_name\")\n",
    "    .sort_values(by=[\"year\", \"institution\"])   # sort by multiple columns\n",
    ")\n",
    "\n",
    "# Export to CSV without the DataFrame index\n",
    "pubs_phd_not_confirmed_df.to_csv(\"data/output/phds_not_confirmed.csv\", index=False)\n",
    "\n",
    "pubs_phd_not_confirmed_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure one row per PhD by dropping duplicate names\n",
    "count_phds_df = extraction_df[\n",
    "    ['phd_name', 'phd_id', 'phd_match_by', 'affiliation_match', 'exact_match']\n",
    "].drop_duplicates(subset='phd_name')\n",
    "\n",
    "n_phds = len(count_phds_df)\n",
    "\n",
    "# Fix downcasting warning by calling infer_objects after fillna for the matching columns\n",
    "count_phds_df['affiliation_match'] = (\n",
    "    count_phds_df['affiliation_match']\n",
    "    .fillna(False)\n",
    ")\n",
    "count_phds_df['exact_match'] = (\n",
    "    count_phds_df['exact_match']\n",
    "    .fillna(False)\n",
    ")\n",
    "\n",
    "# Function to assign a match category based on the flags and phd_id\n",
    "def determine_category(row):\n",
    "    # PhD not found if phd_id is empty or NaN\n",
    "    if pd.isna(row['phd_id']) or row['phd_id'] == '':\n",
    "        return 'Not found'\n",
    "    # If a PhD was found but not confirmed (i.e. no match information)\n",
    "    elif pd.isna(row['phd_match_by']):\n",
    "        return 'Not confirmed'\n",
    "    else:\n",
    "        # Confirmed PhDs: split by exact and affiliation match\n",
    "        if not row['exact_match'] and not row['affiliation_match']:\n",
    "            return 'Fuzzy only'\n",
    "        elif not row['exact_match'] and row['affiliation_match']:\n",
    "            return 'Fuzzy + affiliation'\n",
    "        elif row['exact_match'] and not row['affiliation_match']:\n",
    "            return 'Exact only'\n",
    "        elif row['exact_match'] and row['affiliation_match']:\n",
    "            return 'Exact + affiliation'\n",
    "        else:\n",
    "            return 'Other'\n",
    "\n",
    "# Apply the categorization function to create a new column\n",
    "count_phds_df['match_category'] = count_phds_df.apply(determine_category, axis=1)\n",
    "\n",
    "# Count the number of PhDs per match category\n",
    "match_counts = count_phds_df['match_category'].value_counts()\n",
    "\n",
    "# Calculate number of confirmed PhDs (those falling into the four confirmed categories)\n",
    "confirmed_categories = ['Fuzzy only', 'Fuzzy + affiliation', 'Exact only', 'Exact + affiliation']\n",
    "n_confirmed_phds = count_phds_df['match_category'].isin(confirmed_categories).sum()\n",
    "\n",
    "# Create a bar plot of the categories\n",
    "ax = match_counts.plot(kind='bar')\n",
    "ax.bar_label(ax.containers[0], label_type='edge')\n",
    "\n",
    "# Set labels and titles for clarity\n",
    "plt.xlabel(\"Match Category\")\n",
    "plt.ylabel(\"Number of PhDs\")\n",
    "plt.suptitle(\"PhD Matching Confirmation by Category\", fontsize=12)\n",
    "plt.title(f\"Confirmed {n_confirmed_phds} out of {n_phds} PhDs\", fontsize=10)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out rows where 'phd_id' is Na, so that we only look at PhDs we could confirm\n",
    "count_contrib_df = extraction_df.query('phd_id.notna()')[['contributor_name', 'sup_match_by']]\n",
    "\n",
    "# Replace NaN in 'sup_match_by' with 'Not confirmed'\n",
    "count_contrib_df['sup_match_by'] = count_contrib_df['sup_match_by'].fillna('Not confirmed')\n",
    "\n",
    "# Count how many contributors per match type\n",
    "match_contrib_counts = count_contrib_df['sup_match_by'].value_counts()\n",
    "\n",
    "# Create a bar plot\n",
    "ax = match_contrib_counts.plot(kind='bar', color='#FFD54F')\n",
    "\n",
    "# Add count labels on top of each bar\n",
    "ax.bar_label(ax.containers[0], label_type='edge')\n",
    "\n",
    "# Add labels and title for clarity\n",
    "plt.xlabel(\"Match Type\")\n",
    "plt.ylabel(\"Number of contributors\")\n",
    "plt.suptitle(\"Contributor Matching Confirmation by Type\", fontsize=12) # title\n",
    "plt.title(f\"Only considering the {n_confirmed_phds} PhDs we could confirm\", fontsize=10) # subtile\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
