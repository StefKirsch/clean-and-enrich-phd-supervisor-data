{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open Alex Extraction and Matching with .search()\n",
    "\n",
    "The goal of this Notebook is look up the PhD students (Authors) contained in the [cleaned](clean_data.ipynb) NARCIS dataset, and\n",
    "1. Confirm they can be found in OpenAlex\n",
    "2. Confirm their affiliation in NARCIS matches the one in OpenAlex\n",
    "2. Confirm they wrote the associated PhD Thesis\n",
    "3. Per author, look up all the contributors (i.e. potential first supervisors) that are listen in the NARCIS dataset and\n",
    "    a. Find all authors that have worked for the same organization at the time the PhD thesis was published (within a 1 year window)\n",
    "    b. xxx\n",
    "\n",
    "\n",
    "The previous version of this notebook written by a Bachelor student was using the `.search_filter()` method of `pyalex`, which does not search alternate spellings of the specified name. In this notebook we are using `search_filter()`, which does not have that problem. See the example code [here](search_parameter_vs_search_filter.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyalex import Works, Authors, Sources, Institutions, Topics, Publishers, Funders, Concepts\n",
    "import pyalex # importing full package seems to be the only way to call `pyalex.config.email = email_address`\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from os import path\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from src.unabbreviate_institutions import unabbreviate_institutions\n",
    "from src.open_alex_helpers import AuthorRelations, find_phd_and_supervisors_in_row, get_supervisors_openalex_ids\n",
    "from src.config_selector import get_dataset_selector\n",
    "\n",
    "# Initialize tqdm for progress bars\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and display the selector\n",
    "selector = get_dataset_selector()\n",
    "selector.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NROWS = selector.NROWS\n",
    "use_dataset = selector.use_dataset\n",
    "output_filename = selector.output_filename\n",
    "\n",
    "print(f\"Now using dataset: {use_dataset} with NROWS={NROWS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically reloads any modules that are imported, \n",
    "# so that any changes made to the module files are reflected # without needing to restart the Jupyter kernel.\n",
    "# load autoreload module\n",
    "%load_ext autoreload\n",
    "# mode 1 reloads only when an import statement is called. For production\n",
    "# mode 2 reloads before execution of every cell\n",
    "%autoreload 2\n",
    "\n",
    "# limit the number of rows that are shown with printing data frames\n",
    "pd.set_option('display.max_rows', 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set contact email address to get to use the [polite pool](https://docs.openalex.org/how-to-use-the-api/rate-limits-and-authentication#the-polite-pool). Also, if you are on a premium plan, you can access the higher usage limit by using the associated email address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get contact email address from file\n",
    "email_file_path = 'contact_email.txt'\n",
    "\n",
    "if path.isfile(email_file_path):\n",
    "    with open(email_file_path, 'r') as file:\n",
    "        email_address = file.read().strip()\n",
    "\n",
    "    # Assign the email address to the pyalex configuration\n",
    "    pyalex.config.email = email_address\n",
    "\n",
    "pyalex.config.email"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Cleaned processed NARCIS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if NROWS == None:\n",
    "    output_filename = 'data/output/matched_pairs_full.csv'\n",
    "else:\n",
    "    output_filename = f'data/output/matched_pairs_{NROWS}_rows.csv'\n",
    "\n",
    "if use_dataset == 'biomedical_5y':\n",
    "    # biomedical subset\n",
    "    pubs_df = pd.read_csv('data/cleaned/pubs_biomedical5y.csv')\n",
    "else:\n",
    "    # general dataset\n",
    "    pubs_df = pd.read_csv('data/cleaned/pubs.csv')\n",
    "\n",
    "# Take a sample\n",
    "\n",
    "if NROWS == None:\n",
    "    n_sample = len(pubs_df)\n",
    "else:\n",
    "    n_sample = NROWS\n",
    "\n",
    "pubs_df = pubs_df.sample(n=n_sample, random_state=42).reset_index(drop=True)\n",
    "\n",
    "pubs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace institution abbreviation with names that can be found in OpenAlex\n",
    "pubs_unabbrev_df = unabbreviate_institutions(pubs_df, 'institution')\n",
    "pubs_unabbrev_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Priority supervisor list from ResponsibleSupervision pilot\n",
    "\n",
    "This dataset was created during the Responsible Supervision pilot project, see [here](https://github.com/tamarinde/ResponsibleSupervision/tree/main/Pilot-responsible-supervision)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_url = \"https://github.com/tamarinde/ResponsibleSupervision/tree/main/Pilot-responsible-supervision/data/spreadsheets\"\n",
    "csv_path = \"data/output/sups_pilot.csv\"\n",
    "\n",
    "try:\n",
    "    # Attempt to read the supervisors in the pilot dataset from csv_path\n",
    "    # If it fails, we get them again from GitHub\n",
    "    supervisors_in_pilot_dataset = get_supervisors_openalex_ids(repo_url, csv_path)\n",
    "    print(\"Unique Supervisors with OpenAlex IDs:\")\n",
    "    print(supervisors_in_pilot_dataset)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the pre-trained SPECTER model by allenai (designed for scientific documents). We pre-load the model here, so that we don't need to do that per class instance.\n",
    "\n",
    "Citation information can be found here: https://github.com/allenai/specter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"allenai-specter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the dict to overwrite the default class attribute specified in src/open_alex_helpers.py\n",
    "AuthorRelations.supervisors_in_pilot_dataset = supervisors_in_pilot_dataset\n",
    "\n",
    "# Apply the function to each row with a constant, preloaded model\n",
    "extraction_series = pubs_unabbrev_df.progress_apply(\n",
    "    lambda row: find_phd_and_supervisors_in_row(row, model),\n",
    "    axis=1\n",
    "    )\n",
    "\n",
    "# Concatenate all DataFrames into one\n",
    "extraction_df = pd.concat(list(extraction_series), ignore_index=True)\n",
    "\n",
    "extraction_df.to_csv(output_filename, index=False)\n",
    "\n",
    "extraction_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"We managed to find contributors with {extraction_df['n_shared_inst_grad'].sum()} shared institutions and {extraction_df['n_shared_pubs'].sum()} shared publications!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the extraction dataset from file in case we didn't run the extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'extraction_df' not in locals() and 'extraction_df' not in globals():\n",
    "    file_path = output_filename\n",
    "    \n",
    "    # Check if the file exists\n",
    "    if path.exists(file_path):\n",
    "        extraction_df = pd.read_csv(file_path)\n",
    "        print(f\"Read `extraction_df` from {file_path}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "extraction_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get PhDs that we could not find in OpenAlex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Filter extraction_df for rows with phd_id = NaN\n",
    "extraction_none_df = extraction_df.query(\"phd_id != phd_id\")\n",
    "\n",
    "# Step 2: Filter pubs_unabbrev_df for matching phd_names; then sort and export\n",
    "pubs_phd_not_confirmed_df = (\n",
    "    pubs_unabbrev_df\n",
    "    .query(\"phd_name in @extraction_none_df.phd_name\")\n",
    "    .sort_values(by=[\"year\", \"institution\"])   # sort by multiple columns\n",
    ")\n",
    "\n",
    "# Export to CSV without the DataFrame index\n",
    "pubs_phd_not_confirmed_df.to_csv(\"data/output/phds_not_confirmed.csv\", index=False)\n",
    "\n",
    "pubs_phd_not_confirmed_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure one row per PhD by dropping duplicate names\n",
    "count_phds_df = extraction_df[\n",
    "    ['phd_name', 'phd_id', 'phd_match_by', 'phd_match_score', 'affiliation_match', 'near_exact_match', 'exact_match']\n",
    "].drop_duplicates(subset='phd_name')\n",
    "\n",
    "n_phds = len(count_phds_df)\n",
    "\n",
    "count_phds_df['affiliation_match'] = (\n",
    "    count_phds_df['affiliation_match']\n",
    "    .fillna(False)\n",
    ")\n",
    "count_phds_df['exact_match'] = (\n",
    "    count_phds_df['exact_match']\n",
    "    .fillna(False)\n",
    ")\n",
    "\n",
    "# Function to assign a match category based on the flags and phd_id\n",
    "def determine_category_phd(row):\n",
    "    # If a PhD was found but not confirmed (i.e. no match information)\n",
    "    if pd.isna(row['phd_match_by']):\n",
    "        return 'Not confirmed'\n",
    "    else:\n",
    "        # Confirmed PhDs: split by exact and affiliation match\n",
    "        if not row['phd_match_score']:\n",
    "            return 'Name match, no other match'\n",
    "        if not row['exact_match'] and not row['affiliation_match'] and not row['near_exact_match']:\n",
    "            return 'Fuzzy only'\n",
    "        elif not row['exact_match'] and row['affiliation_match'] and not row['near_exact_match']:\n",
    "            return 'Fuzzy + affiliation'\n",
    "        elif row['near_exact_match'] and not row['affiliation_match'] and not row['exact_match']:\n",
    "            return 'Near exact only'\n",
    "        elif row['near_exact_match'] and row['affiliation_match'] and not row['exact_match']:\n",
    "            return 'Near exact + affiliation'\n",
    "        elif row['exact_match'] and not row['affiliation_match']:\n",
    "            return 'Exact only'\n",
    "        elif row['exact_match'] and row['affiliation_match']:\n",
    "            return 'Exact + affiliation'\n",
    "        else:\n",
    "            return 'Other'\n",
    "\n",
    "# Apply the categorization function to create a new column\n",
    "count_phds_df['match_category'] = count_phds_df.apply(determine_category_phd, axis=1)\n",
    "\n",
    "bar_categories = [\n",
    "    'Not confirmed',\n",
    "    'Name match, no other match',\n",
    "    'Fuzzy only',\n",
    "    'Fuzzy + affiliation',\n",
    "    'Near exact only',\n",
    "    'Near exact + affiliation',\n",
    "    'Exact only',\n",
    "    'Exact + affiliation'\n",
    "    ]\n",
    "confirmed_categories = bar_categories[1:]\n",
    "\n",
    "\n",
    "# Count the number of PhDs per match category\n",
    "match_counts = count_phds_df['match_category'].value_counts().reindex(bar_categories)\n",
    "\n",
    "# Calculate number of confirmed PhDs (those falling into the four confirmed categories)\n",
    "n_confirmed_phds = count_phds_df['match_category'].isin(confirmed_categories).sum()\n",
    "\n",
    "# Create a bar plot of the categories\n",
    "ax = match_counts.plot(kind='bar')\n",
    "ax.bar_label(ax.containers[0], label_type='edge')\n",
    "\n",
    "# Set labels and titles for clarity\n",
    "plt.xlabel(\"Match Category\")\n",
    "plt.ylabel(\"Number of PhDs\")\n",
    "plt.suptitle(\"PhD Matching Confirmation by Category\", fontsize=12)\n",
    "plt.title(f\"Confirmed {n_confirmed_phds} out of {n_phds} PhDs\", fontsize=10)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_phds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extraction_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to assign a match category based on the flags\n",
    "def determine_category_contrib(row):\n",
    "    # If a contributor was found but not confirmed (i.e. no match information)\n",
    "    if not bool(row['contributor_id']): # returns False for None and zero length\n",
    "        return 'Not found/no affiliation match'\n",
    "    else:\n",
    "        # Confirmed contributors\n",
    "        if row['contributor_confirmed']:\n",
    "            return 'Found and confirmed'\n",
    "        elif not row['contributor_confirmed'] and row['contributor_id']:\n",
    "            return 'Found, but not confirmed'\n",
    "        else:\n",
    "            return 'Other'\n",
    "\n",
    "bar_categories = [\n",
    "    #'Other',\n",
    "    'Not found/no affiliation match',\n",
    "    'Found, but not confirmed',\n",
    "    'Found and confirmed'\n",
    "]\n",
    "\n",
    "# Apply the categorization function to create a new column\n",
    "extraction_df['match_category'] = extraction_df.apply(determine_category_contrib, axis=1)\n",
    "\n",
    "# filter out rows where 'phd_id' is Na, so that we only look at PhDs we could confirm\n",
    "count_contrib_df = extraction_df.query('phd_id.notna()')[['contributor_name', 'match_category']]\n",
    "\n",
    "# Count how many contributors per match type\n",
    "match_contrib_counts = count_contrib_df['match_category'].value_counts().reindex(bar_categories)\n",
    "\n",
    "# Create a bar plot\n",
    "ax = match_contrib_counts.plot(kind='bar', color='#FFD54F')\n",
    "\n",
    "# Add count labels on top of each bar\n",
    "ax.bar_label(ax.containers[0], label_type='edge')\n",
    "\n",
    "# Add labels and title for clarity\n",
    "plt.xlabel(\"Match Type\")\n",
    "plt.ylabel(\"Number of contributors\")\n",
    "plt.suptitle(\"Contributor Matching Confirmation by Type\", fontsize=12) # title\n",
    "plt.title(f\"Only considering the {n_confirmed_phds} PhDs we could confirm\", fontsize=10) # subtile\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
