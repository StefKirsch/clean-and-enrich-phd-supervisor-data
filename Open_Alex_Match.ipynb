{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open Alex Extraction and Matching with .search()\n",
    "\n",
    "The goal of this Notebook is look up the PhD students (Authors) contained in the [cleaned](clean_data.ipynb) NARCIS dataset, and\n",
    "1. Confirm they can be found in OpenAlex\n",
    "2. Confirm their affiliation in NARCIS matches the one in OpenAlex\n",
    "2. Confirm they wrote the associated PhD Thesis\n",
    "3. Per author, look up all the contributors (i.e. potential first supervisors) that are listen in the NARCIS dataset and\n",
    "    a. Find all authors that have worked for the same organization at the time the PhD thesis was published (within a 1 year window)\n",
    "    b. xxx\n",
    "\n",
    "\n",
    "The previous version of this notebook written by a Bachelor student was using the `.search_filter()` method of `pyalex`, which does not search alternate spellings of the specified name. In this notebook we are using `search_filter()`, which does not have that problem. See the example code [here](search_parameter_vs_search_filter.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyalex import Works, Authors, Sources, Institutions, Topics, Publishers, Funders, Concepts\n",
    "from pyalex import config # to set email_address\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from os import path\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from src.unabbreviate_institutions import unabbreviate_institutions\n",
    "from src.open_alex_helpers import AuthorRelations, find_phd_and_supervisors_in_row, get_supervisors_openalex_ids\n",
    "from src.dataset_config_helpers import read_config, load_dataset\n",
    "from src.api_cache_helpers import initialize_request_cache\n",
    "from src.plotters import PhDMatchPlotter\n",
    "\n",
    "# Initialize tqdm for progress bars\n",
    "tqdm.pandas()\n",
    "\n",
    "# Install the cache before any API calls are made.\n",
    "# This will cache every API call to Open Alex and if a cached version of the call is available,\n",
    "# it will be preferred over making a new API call.\n",
    "initialize_request_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically reloads any modules that are imported, \n",
    "# so that any changes made to the module files are reflected # without needing to restart the Jupyter kernel.\n",
    "# load autoreload module\n",
    "%load_ext autoreload\n",
    "# mode 1 reloads only when an import statement is called. For production\n",
    "# mode 2 reloads before execution of every cell\n",
    "%autoreload 2\n",
    "\n",
    "# limit the number of rows that are shown with printing data frames\n",
    "pd.set_option('display.max_rows', 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set contact email address to get to use the [polite pool](https://docs.openalex.org/how-to-use-the-api/rate-limits-and-authentication#the-polite-pool). Also, if you are on a premium plan, you can access the higher usage limit by using the associated email address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get contact email address from file\n",
    "email_file_path = 'contact_email.txt'\n",
    "\n",
    "if path.isfile(email_file_path):\n",
    "    with open(email_file_path, 'r') as file:\n",
    "        email_address = file.read().strip()\n",
    "\n",
    "    # Assign the email address to the pyalex configuration\n",
    "    config.email = email_address\n",
    "\n",
    "config.email"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure number of retries and backoff factor\n",
    "Pyalex is using [urllib3.util.Retry](https://urllib3.readthedocs.io/en/stable/reference/urllib3.util.html) for retrying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.max_retries = 7\n",
    "config.retry_backoff_factor = 2 # conservative backoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Cleaned processed NARCIS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'data/cleaned/pubs_with_domain.csv'\n",
    "output_filename = 'data/output/author_relations.csv'\n",
    "\n",
    "config = read_config('dataset_config.yaml')\n",
    "\n",
    "pubs_df = load_dataset(config, dataset_path)\n",
    "\n",
    "pubs_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace institution abbreviation with names that can be found in OpenAlex\n",
    "pubs_unabbrev_df = unabbreviate_institutions(pubs_df, 'institution')\n",
    "pubs_unabbrev_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PhD candidates with 4 or more supervisors (for information)\n",
    "contributor_cols = [f'contributor_{i}' for i in range(1, 11)]\n",
    "\n",
    "# Count non-missing contributor entries per row\n",
    "pubs_unabbrev_df['contributor_count'] = pubs_unabbrev_df[contributor_cols].notna().sum(axis=1)\n",
    "\n",
    "# Reorder columns to place contributor_count after institution\n",
    "cols = list(pubs_unabbrev_df.columns)\n",
    "if 'institution' in cols and 'contributor_count' in cols:\n",
    "    cols.remove('contributor_count')\n",
    "    institution_index = cols.index('institution')\n",
    "    cols.insert(institution_index + 1, 'contributor_count')\n",
    "    pubs_unabbrev_df = pubs_unabbrev_df[cols]\n",
    "\n",
    "# Filter and sort\n",
    "pubs_more_than_n_df = (\n",
    "    pubs_unabbrev_df[pubs_unabbrev_df['contributor_count'] >= 4]\n",
    "    .sort_values(by=['institution', 'contributor_count'], ascending=[True, True])\n",
    "    .copy()\n",
    ")\n",
    "\n",
    "print(f\"There are {pubs_more_than_n_df.shape[0]} PhD candidates with 4 or more supervisors\")\n",
    "\n",
    "pubs_more_than_n_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Priority supervisor list from ResponsibleSupervision pilot\n",
    "\n",
    "This dataset was created during the Responsible Supervision pilot project, see [here](https://github.com/tamarinde/ResponsibleSupervision/tree/main/Pilot-responsible-supervision)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_url = \"https://github.com/tamarinde/ResponsibleSupervision/tree/main/Pilot-responsible-supervision/data/spreadsheets\"\n",
    "csv_path = \"data/output/sups_pilot.csv\"\n",
    "\n",
    "try:\n",
    "    # Attempt to read the supervisors in the pilot dataset from csv_path\n",
    "    # If it fails, we get them again from GitHub\n",
    "    supervisors_in_pilot_dataset = get_supervisors_openalex_ids(repo_url, csv_path)\n",
    "    print(\"Unique Supervisors with OpenAlex IDs:\")\n",
    "    print(supervisors_in_pilot_dataset)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the pre-trained SPECTER model by allenai (designed for scientific documents). We pre-load the model here, so that we don't need to do that per class instance.\n",
    "\n",
    "Citation information can be found here: https://github.com/allenai/specter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"allenai-specter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the dict to overwrite the default class attribute specified in src/open_alex_helpers.py\n",
    "AuthorRelations.supervisors_in_pilot_dataset = supervisors_in_pilot_dataset\n",
    "\n",
    "# Apply the function to each row with a constant, preloaded model\n",
    "extraction_series = pubs_unabbrev_df.progress_apply(\n",
    "    lambda row: find_phd_and_supervisors_in_row(row, model),\n",
    "    axis=1\n",
    "    )\n",
    "\n",
    "# Concatenate all DataFrames into one\n",
    "extraction_df = pd.concat(list(extraction_series), ignore_index=True)\n",
    "\n",
    "extraction_df.to_csv(output_filename, index=False)\n",
    "\n",
    "extraction_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"We managed to find contributors with {extraction_df['n_shared_inst_grad'].sum()} shared institutions and {extraction_df['n_shared_pubs'].sum()} shared publications!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the extraction dataset from file in case we didn't run the extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'extraction_df' not in locals() and 'extraction_df' not in globals():\n",
    "    file_path = output_filename\n",
    "    \n",
    "    # Check if the file exists\n",
    "    if path.exists(file_path):\n",
    "        extraction_df = pd.read_csv(file_path)\n",
    "        print(f\"Read `extraction_df` from {file_path}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "extraction_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get PhDs that we could not find in OpenAlex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Filter extraction_df for rows with phd_id = NaN\n",
    "extraction_none_df = extraction_df.query(\"phd_id != phd_id\")\n",
    "\n",
    "# Step 2: Filter pubs_unabbrev_df for matching phd_names; then sort and export\n",
    "pubs_phd_not_confirmed_df = (\n",
    "    pubs_unabbrev_df\n",
    "    .query(\"phd_name in @extraction_none_df.phd_name\")\n",
    "    .sort_values(by=[\"year\", \"institution\"])   # sort by multiple columns\n",
    ")\n",
    "\n",
    "# Export to CSV without the DataFrame index\n",
    "pubs_phd_not_confirmed_df.to_csv(\"data/output/phds_not_confirmed.csv\", index=False)\n",
    "\n",
    "pubs_phd_not_confirmed_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter = PhDMatchPlotter(extraction_df)\n",
    "ax = plotter.plot()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_phds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extraction_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to assign a match category based on the flags\n",
    "def determine_category_contrib(row):\n",
    "    # If a contributor was found but not confirmed (i.e. no match information)\n",
    "    if pd.isnull(row['contributor_id']) or not row['contributor_id']:\n",
    "        return 'Not found'\n",
    "    else:\n",
    "        if not row['n_shared_pubs'] and not row['same_grad_inst']:\n",
    "            return 'No shared publications or affiliation at graduation'\n",
    "        elif not row['n_shared_pubs'] and row['same_grad_inst']:\n",
    "            return 'Shared affiliation at graduation only'\n",
    "        elif row['n_shared_pubs'] and not row['same_grad_inst']:\n",
    "            return 'Shared publications only'\n",
    "        elif row['n_shared_pubs'] and row['same_grad_inst']:\n",
    "            return 'Shared publications and affiliation at graduation'\n",
    "        else:\n",
    "            return 'Other'\n",
    "\n",
    "bar_categories = [\n",
    "    'Not found',\n",
    "    'No shared publications or affiliation at graduation',\n",
    "    'Shared affiliation at graduation only',\n",
    "    'Shared publications only',\n",
    "    'Shared publications and affiliation at graduation',\n",
    "    'Other'\n",
    "]\n",
    "\n",
    "# Apply the categorization function to create a new column\n",
    "extraction_df['match_category'] = extraction_df.apply(determine_category_contrib, axis=1)\n",
    "\n",
    "# filter out rows where 'phd_id' is Na, so that we only look at PhDs we could confirm\n",
    "count_contrib_df = extraction_df.query('phd_id.notna()')[['contributor_name', 'match_category']].copy()\n",
    "\n",
    "# Count how many contributors per match type\n",
    "match_contrib_counts = count_contrib_df['match_category'].value_counts().reindex(bar_categories)\n",
    "\n",
    "# Create a bar plot\n",
    "ax = match_contrib_counts.plot(kind='bar', color='#FFD54F')\n",
    "\n",
    "# Add count labels on top of each bar\n",
    "ax.bar_label(ax.containers[0], label_type='edge')\n",
    "\n",
    "# Add labels and title for clarity\n",
    "plt.xlabel(\"Match Type\")\n",
    "plt.ylabel(\"Number of contributors\")\n",
    "plt.suptitle(\"Contributor Matching Confirmation by Type\", fontsize=12) # title\n",
    "plt.title(f\"Only considering the {n_confirmed_phds} PhDs we could confirm\", fontsize=10) # subtile\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
