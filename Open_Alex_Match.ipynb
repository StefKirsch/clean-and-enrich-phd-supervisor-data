{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open Alex Extraction and Matching with .search()\n",
    "\n",
    "The goal of this Notebook is look up the PhD students (Authors) contained in the [cleaned](clean_data.ipynb) NARCIS dataset, and\n",
    "1. Confirm they can be found in OpenAlex\n",
    "2. Confirm their affiliation in NARCIS matches the one in OpenAlex\n",
    "2. Confirm they wrote the associated PhD Thesis\n",
    "3. Per author, look up all the contributors (i.e. potential first supervisors) that are listen in the NARCIS dataset and\n",
    "    a. Find all authors that have worked for the same organization at the time the PhD thesis was published (within a 1 year window)\n",
    "    b. xxx\n",
    "\n",
    "\n",
    "The previous version of this notebook written by a Bachelor student was using the `.search_filter()` method of `pyalex`, which does not search alternate spellings of the specified name. In this notebook we are using `search_filter()`, which does not have that problem. See the example code [here](search_parameter_vs_search_filter.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyalex import Works, Authors, Sources, Institutions, Topics, Publishers, Funders, Concepts\n",
    "import pyalex # importing full package seems to be the only way to call `pyalex.config.email = email_address`\n",
    "import pandas as pd\n",
    "from os import path\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from src.unabbreviate_institutions import unabbreviate_institutions\n",
    "from src.open_alex_helpers import AuthorRelations, find_phd_and_supervisors_in_row, get_supervisors_openalex_ids\n",
    "from src.io_helpers import remove_illegal_title_characters\n",
    "from src.clean_names_helpers import format_name_to_lastname_firstname\n",
    "from src.config_selector import get_dataset_selector\n",
    "\n",
    "# Initialize tqdm for progress bars\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and display the selector\n",
    "selector = get_dataset_selector()\n",
    "selector.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NROWS = selector.NROWS\n",
    "use_dataset = selector.use_dataset\n",
    "output_filename = selector.output_filename\n",
    "\n",
    "print(f\"Now using dataset: {use_dataset} with NROWS={NROWS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify PhD matching criterion\n",
    "criterion=\"either\"\n",
    "\n",
    "# the contributor matching criterion right now is hard-coded to same 'institution at graduation'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically reloads any modules that are imported, \n",
    "# so that any changes made to the module files are reflected # without needing to restart the Jupyter kernel.\n",
    "# load autoreload module\n",
    "%load_ext autoreload\n",
    "# mode 1 reloads only when an import statement is called. For production\n",
    "# mode 2 reloads before execution of every cell\n",
    "%autoreload 2\n",
    "\n",
    "# limit the number of rows that are shown with printing data frames\n",
    "pd.set_option('display.max_rows', 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set contact email address to get to use the [polite pool](https://docs.openalex.org/how-to-use-the-api/rate-limits-and-authentication#the-polite-pool). Also, if you are on a premium plan, you can access the higher usage limit by using the associated email address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get contact email address from file\n",
    "email_file_path = 'contact_email.txt'\n",
    "\n",
    "if path.isfile(email_file_path):\n",
    "    with open(email_file_path, 'r') as file:\n",
    "        email_address = file.read().strip()\n",
    "\n",
    "    # Assign the email address to the pyalex configuration\n",
    "    pyalex.config.email = email_address\n",
    "\n",
    "pyalex.config.email"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Cleaned processed NARCIS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if NROWS == None:\n",
    "    output_filename = 'data/output/matched_pairs_full.csv'\n",
    "else:\n",
    "    output_filename = f'data/output/matched_pairs_{NROWS}_rows.csv'\n",
    "\n",
    "if use_dataset == 'biomedical_5y':\n",
    "    # biomedical subset\n",
    "    pubs_df = pd.read_csv('data/cleaned/pubs_biomedical5y.csv')\n",
    "else:\n",
    "    # general dataset\n",
    "    pubs_df = pd.read_csv('data/cleaned/pubs.csv')\n",
    "\n",
    "# Take a sample\n",
    "\n",
    "if NROWS == None:\n",
    "    n_sample = len(pubs_df)\n",
    "else:\n",
    "    n_sample = NROWS\n",
    "\n",
    "pubs_df = pubs_df.sample(n=n_sample, random_state=42).reset_index(drop=True)\n",
    "\n",
    "pubs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need some additional data preparation and cleaning to make sure the dataset interacts well with OpenAlex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace institution abbreviation with names that can be found in OpenAlex\n",
    "pubs_unabbrev_df = unabbreviate_institutions(pubs_df, 'institution')\n",
    "pubs_unabbrev_df\n",
    "\n",
    "# remove illegal characters from title that could trip up the API\n",
    "pubs_unabbrev_df[\"title\"] = pubs_unabbrev_df[\"title\"].apply(remove_illegal_title_characters)\n",
    "pubs_unabbrev_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Priority supervisor list from ResponsibleSupervision pilot\n",
    "\n",
    "This dataset was created during the Responsible Supervision pilot project, see [here](https://github.com/tamarinde/ResponsibleSupervision/tree/main/Pilot-responsible-supervision)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_url = \"https://github.com/tamarinde/ResponsibleSupervision/tree/main/Pilot-responsible-supervision/data/spreadsheets\"\n",
    "csv_path = \"data/output/sups_pilot.csv\"\n",
    "\n",
    "try:\n",
    "    # Attempt to read the supervisors in the pilot dataset from csv_path\n",
    "    # If it fails, we get them again from GitHub\n",
    "    supervisors_in_pilot_dataset = get_supervisors_openalex_ids(repo_url, csv_path)\n",
    "    print(\"Unique Supervisors with OpenAlex IDs:\")\n",
    "    print(supervisors_in_pilot_dataset)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the dict to overwrite the default class attribute specified in src/open_alex_helpers.py\n",
    "AuthorRelations.supervisors_in_pilot_dataset = supervisors_in_pilot_dataset\n",
    "\n",
    "# Apply the function to each row\n",
    "extraction_series = pubs_unabbrev_df.progress_apply(\n",
    "    find_phd_and_supervisors_in_row,\n",
    "    axis=1,\n",
    "    args=(criterion,) # PhD matching criterion\n",
    ")\n",
    "\n",
    "# Concatenate all DataFrames into one\n",
    "extraction_df = pd.concat(list(extraction_series), ignore_index=True)\n",
    "\n",
    "extraction_df.to_csv(output_filename, index=False)\n",
    "\n",
    "extraction_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analysis and Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the extraction dataset from file in case we didn't run the extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'extraction_df' not in locals() and 'extraction_df' not in globals():\n",
    "    file_path = output_filename\n",
    "    \n",
    "    # Check if the file exists\n",
    "    if path.exists(file_path):\n",
    "        extraction_df = pd.read_csv(file_path)\n",
    "        print(f\"Read `extraction_df` from {file_path}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "extraction_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get PhDs that we could not find in OpenAlex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Filter extraction_df for rows with phd_id = NaN\n",
    "extraction_none_df = extraction_df.query(\"phd_id != phd_id\")\n",
    "\n",
    "# Step 2: Filter pubs_unabbrev_df for matching phd_names; then sort and export\n",
    "pubs_phd_not_confirmed_df = (\n",
    "    pubs_unabbrev_df\n",
    "    .query(\"phd_name in @extraction_none_df.phd_name\")\n",
    "    .sort_values(by=[\"year\", \"institution\"])   # sort by multiple columns\n",
    ")\n",
    "\n",
    "# Export to CSV without the DataFrame index\n",
    "pubs_phd_not_confirmed_df.to_csv(\"data/output/phds_not_confirmed.csv\", index=False)\n",
    "\n",
    "pubs_phd_not_confirmed_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure only one row per PhD for counting how they were confirmed\n",
    "count_phds_df = extraction_df[['phd_name', 'phd_match_by']].drop_duplicates(subset='phd_name')\n",
    "\n",
    "n_phds = len(count_phds_df)\n",
    "n_confirmed_phds = len(count_phds_df.query('phd_match_by.notna()'))\n",
    "\n",
    "# Replace NaN in 'phd_match_by' with 'Not confirmed'\n",
    "count_phds_df['phd_match_by'] = count_phds_df['phd_match_by'].fillna('Not confirmed')\n",
    "\n",
    "# Count how many PhDs per match type\n",
    "match_phds_counts = count_phds_df['phd_match_by'].value_counts()\n",
    "\n",
    "# Create a bar plot\n",
    "ax = match_phds_counts.plot(kind='bar')\n",
    "\n",
    "# Add count labels on top of each bar\n",
    "ax.bar_label(ax.containers[0], label_type='edge')\n",
    "\n",
    "# Add a legend specifying the matching criterion\n",
    "ax.legend([f'PhD matching by \"{criterion}\"'], loc=\"upper right\")\n",
    "\n",
    "# Add labels and title for clarity\n",
    "plt.xlabel(\"Match Type\")\n",
    "plt.ylabel(\"Number of PhDs\")\n",
    "plt.suptitle(\"PhD Matching Confirmation by Type\",fontsize=12) # title\n",
    "plt.title(f\"Confirmed {n_confirmed_phds} out of {n_phds} PhDs. That is {round(n_confirmed_phds/n_phds*100, 2)}%\", fontsize=10) # subtitle\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out rows where 'phd_id' is Na, so that we only look at PhDs we could confirm\n",
    "count_contrib_df = extraction_df.query('phd_id.notna()')[['contributor_name', 'sup_match_by']]\n",
    "\n",
    "# Replace NaN in 'sup_match_by' with 'Not confirmed'\n",
    "count_contrib_df['sup_match_by'] = count_contrib_df['sup_match_by'].fillna('Not confirmed')\n",
    "\n",
    "# Count how many contributors per match type\n",
    "match_contrib_counts = count_contrib_df['sup_match_by'].value_counts()\n",
    "\n",
    "# Create a bar plot\n",
    "ax = match_contrib_counts.plot(kind='bar', color='#FFD54F')\n",
    "\n",
    "# Add count labels on top of each bar\n",
    "ax.bar_label(ax.containers[0], label_type='edge')\n",
    "\n",
    "# Add labels and title for clarity\n",
    "plt.xlabel(\"Match Type\")\n",
    "plt.ylabel(\"Number of contributors\")\n",
    "plt.suptitle(\"Contributor Matching Confirmation by Type\", fontsize=12) # title\n",
    "plt.title(f\"Only considering the {n_confirmed_phds} PhDs we could confirm\", fontsize=10) # subtile\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
